---
title: "analyzeBackwardsPaper2 E2"
author: "Alex Holcombe"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE,  comment = NA)
```

MATLAB code ran the experiment.
.mat file been preprocessed into melted long R dataframe by WHAT?

Import the dataframe.

```{r importData, echo=FALSE, message=FALSE}
#rm(list=ls())

#Import raw data
rawDataFileWithPath<- file.path('data','P2E2_PilotData.Rdata') 
if (file.exists(rawDataFileWithPath)) {
  dg<- readRDS( rawDataFileWithPath ) 
} else {
  print(cat("Error! Could not find file",rawDataFileWithPath," Probably your working directory is bad:"))
  print(getwd())
}

mixModelingPath<- file.path("mixtureModeling")

numItemsInStream<- length( dg$letterSeq[1,] )  

#It seems that to work with dplyr, can't have array field like letterSeq (letterSeq indicates what each item in the stream of each trial was)
dg$letterSeq<- NULL

head(dg)
```


Fit mixture model with R (if fitting not previously done and cached), print first several parameter estimates: 
```{r, echo=FALSE, message=FALSE, cache=TRUE}
#Setting cache = TRUE in hopes won't have to recalculate estimates "if cached results exist and this code chunk has not been changed since last run" 

library(dplyr)
#Load files containing the functions needed to fit mixture model to data
source( file.path(mixModelingPath,'analyzeOneCondition.R') )
source( file.path(mixModelingPath,'parameterBounds.R') )

condtnVariableNames <- c("subject","condition", "target") # 

#Check whether already have parameter estimates or instead need to do it
calculate<-FALSE
if (!exists("estimates")) { 
  calculate<-TRUE 
}

if (calculate) {
  estimates<- dg %>%  #  filter(subject=="CB") %>%
    group_by_(.dots = condtnVariableNames) %>%  #.dots needed when you have a variable containing multiple factor names
    do(analyzeOneCondition(.,numItemsInStream,parameterBounds()))
  estimates<- estimates %>% rename(efficacy = p1, latency = p2, precision = p3)
}
head(estimates)
```

Plot the histogram and fit for one subject*condition.

* yellow = guessing component 
* light blue = Gaussian component
* green = sum of the guessing and Gaussian components. In other words, the histogram heights predicted by the model
* dark blue = continuous Gaussian. This helps get a sense of the effect of discretising the Gaussian. For instance, it's possible  for the Gaussian peak to rise high above the bars and still fit the discrete bins, suggesting  undesirably high estimates of the efficacy (likely accompanied by an undesirably low precision)

For likelihood, lower neg log likelihood means better fit.


```{r plotOneS, echo=FALSE, message=FALSE}

dCB<- dplyr::filter(dg,subject=="CB",condition==2,target==1)  
minSPE<- -11; maxSPE<- 11
plotContinuousGaussian<-TRUE; annotateIt<-TRUE
source(file.path(mixModelingPath,"plotHistWithFit.R"))
plotHistWithFit(dCB,minSPE,maxSPE,dCB$targetSP,numItemsInStream,plotContinuousGaussian,annotateIt)
```


Plot all data. First column is left stream. Second column is right stream. Each subject gets a pair of rows, "1" for upright letters, "2" for backwards letters?

```{r, echo=FALSE, message=TRUE, fig.height=36, fig.width=10}
#want fig.height of 10 per subject

source( file.path(mixModelingPath,"calcCurvesDataframes.R") )
library(dplyr)
#create R curves
df<- dg # %>%  dplyr::filter(subject <="BB") #dplyr::filter(subject <= "BD" & subject >="AP")
#dplyr::filter(subject=="AA",orientation=="Canonical")
#source( file.path(mixModelingPath,"histogramPlotting.R") ) #for calcFitDataframes
source( file.path(mixModelingPath,"theme_apa.R") )

#Add R parameter estimates to dataframe
df<- merge(df,estimates) 

curves<- df %>% group_by_at(.vars = condtnVariableNames) %>% 
  do(calcCurvesDataframes(.,minSPE,maxSPE,numItemsInStream))

#Calc numObservations to each condition. This is needed only for scaling the fine-grained Gaussian
#Calc the number of observations for each condition, because gaussianScaledforData needs to know.
dfGroups<- df %>% group_by_at(.vars = condtnVariableNames) %>% summarise(nPerCond = n())
#add nPerCond back to parameter estimates
estimates<- merge(estimates,dfGroups)
grain<-.05
gaussFine<- estimates %>% group_by_at(.vars = condtnVariableNames) %>% do(
  gaussianScaledFromDataframe(.,minSPE,maxSPE,grain) )


#PLOT EVERYTHING
g=ggplot(df, aes(x=SPE)) + facet_grid(subject+condition~target,  scales="free_y")
g<-g+geom_histogram(binwidth=1,color="grey90") + xlim(minSPE,maxSPE)
g<-g +theme_apa() #+theme(panel.grid.minor=element_blank(),panel.grid.major=element_blank())# hide all gridlines.
#g<-g+ theme(line=element_blank(), panel.border = element_blank())
sz=.8
#Plot the underlying Gaussian , not just the discretized Gaussian. But it's way too tall. I don't know if this is 
#a scaling problem or what actually is going on.
#g<-g + geom_line(data=gaussFine,aes(x=x,y=gaussianFreq),color="darkblue",size=1.2)

g<-g+ geom_point(data=curves,aes(x=x,y=combinedFitFreq),color="chartreuse3",size=sz*2.5)
g<-g+ geom_line(data=curves,aes(x=x,y=guessingFreq),color="yellow",size=sz)
#Discretized Gaussian
g<-g+ geom_line(data=curves,aes(x=x,y=gaussianFreq),color="lightblue",size=sz)

numGroups<- length(table(df$condition,df$subject,df$target))
fontSz = 5 #100/numGroups
g<-g + geom_text(data=curves,aes(x=-9,y=32, label = paste("--logLik==", round(val,1), sep = "")),  parse = TRUE,size=fontSz) +
  geom_text(data=curves,aes(x=-7,y=28, label = paste("plain(e)==", round(efficacy,2), sep = "")),  parse = TRUE,size=fontSz) +
  geom_text(data=curves,aes(x=-7,y=25, label = paste("mu==", round(latency,2), sep = "")),  parse = TRUE,size=fontSz)+
  geom_text(data=curves,aes(x=-7,y=22, label = paste("sigma==", round(precision,2), sep = "")),  parse = TRUE,size=fontSz)
show(g)
```

