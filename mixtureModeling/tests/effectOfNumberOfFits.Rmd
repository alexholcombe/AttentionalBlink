---
title: "effect of number of fits"
author: "Alex Holcombe"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import data

```{r importData, echo=FALSE, message=FALSE}
#rm(list=ls())

#Compensate for path getting set to mixtureModeling/tests/
if (basename(getwd()) != "tests") {
  pathNeeded<- "mixtureModeling"
} else { 
  pathNeeded <- ".." 
}

data<- readRDS( file.path(pathNeeded,"tests", "alexImportBackwardsPaper2E1.Rdata") ) #.mat file been preprocessed into melted long dataframe
```


```{r fit, echo=FALSE, message=FALSE}

library(dplyr)
numItemsInStream<- length( data$letterSeq[1,] )  
df<-data
#It seems that to work with dplyr, can't have array field like letterSeq
df$letterSeq<- NULL
dg<- dplyr::filter(df,subject=="AA",condition==1,target==1)  #subject < "AC") #Otherwise will take long time to run the preliminary examples 

source(file.path(pathNeeded,"analyzeOneCondition.R"))
source(file.path(pathNeeded,"parameterBounds.R"))

numReplicates<-1
estimates<-dg %>% group_by(subject,target,condition) %>% 
  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )
```

Here are the estimates from one run for the one data sample:
```{r estimates, echo=FALSE, message=FALSE}
print(estimates)
```

Calculate the standard deviation of the fits for just one sample of data. The variation is remarkably low, except for precision?

```{r sd of fits, echo=FALSE, message=FALSE, cache=TRUE}

#For each condition, do the fit many times and calculate the standard deviation
#Maybe repeat the raw data dataframe as many times as want to fit. Then group by *replicate* and so get
#one row for each replicate fit.

reps<-20
#replicate raw data
dn<- df[rep(1:nrow(df),times = reps), ]
#add a rep field indicating replicate number
dn$replicate <- rep(1:reps, each=nrow(df))

numReplicates<-1 #Only fit the model once because we are manipulating number of fits with reps
estimates<-dn %>% group_by(replicate) %>% 
  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )

estimates<- estimates %>% rename(efficacy = p1, latency = p2, precision = p3)

#it automatically groups it using the tibble group specified variable, so get rid of that by changing to dataframe
estimates<-data.frame(estimates)
roundMean<- function(x) { round(mean(x),2) } #round the means so not so long to read
w<-estimates %>%  summarise_at( .vars = c("efficacy", "latency","precision"),
                                .funs = c(Mean="roundMean", Sd="sd") )
print(w)

```


Do it for each of many conditions to make sure it is true for each condition. The variation for each condition is:

```{r many conditions, echo=FALSE, message=FALSE}

#dg<-filter(df, subject<"AC")

reps<-100
#replicate raw data, creating reps copies of each condition*subject*
dn<- df[rep(1:nrow(df),times = reps), ]
#add a rep field indicating replicate number
dn$replicate <- rep(1:reps, each=nrow(df))

numReplicates<-1 #Only fit the model once per function call because we are manipulating number of fits with reps
###########
#Fit the data, which could take a very long time
many<-dn %>% group_by(replicate,subject,target,condition) %>% 
  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )

#it automatically groups it using the tibble group specified variable, so get rid of that by changing to dataframe
many<-data.frame(many)
many<- many %>% rename(efficacy = p1, latency = p2, precision = p3)

#Calculate the mean and SD across all the replicates
w<-many %>% group_by(subject,target,condition) %>% summarise_at( .vars = c("efficacy", "latency","precision"),
                                .funs = c(Mean="roundMean", Sd="sd") )
#it automatically groups it using the tibble group specified variable, so get rid of that by changing to dataframe
w<-data.frame(w)

print( select(w, -ends_with("Mean")) ) #Don't print the means as then can't see important stuff in single line
```

The mean sds are:
```{r mean sds, echo=FALSE, message=FALSE}

averageSD<- w %>% select(-ends_with("Mean")) %>% summarise_at( .vars = c("efficacy_Sd", "latency_Sd","precision_Sd"),
                                .funs = c(Mean="mean") )

print(averageSD)
```
You'll notice they are large for latency and precision. If results of fits were normally distributed, standard error when `r sampleSizes<-c(5,10,20,50)` 5, 10, 20, and 50 fits are done would be:

```{r mean sds, echo=FALSE, message=FALSE}
sampleSizes<-c(5,10,20,50)
SEnormalCalc<- averageSD[rep(1:nrow(averageSD),times = length(sampleSizes)), ]
SEnormalCalc$sampleSize <- sampleSizes
SEnormalCalc<-SEnormalCalc %>% mutate_each(funs(SE = ./sqrt(sampleSize)), -sampleSize)
SEnormalCalc[,4:7]
```

Examine the empirical SE by looking at the SD of the mean for different numbers of fits.

In other words for each `n` up to `reps`, take random samples of n from the replicates and
calculate the mean. We want to know the sd of the sampling distribution of that mean (the expected se). So we repeat the sampling many times and calculate the mean each time, and the sd of all those means.

Do it separately for each condition and then average the SEs of the conditions.


```{r empirical SE, echo=FALSE, message=FALSE}

#calculate the sd for those 
dfThis<-many

these<- filter(dfThis, replicate == whichFits) #grab those particular fits
#Calculate means with these fits
meanThese<- these %>% group_by(subject,target,condition) %>%
          summarise_at( .vars = c("efficacy", "latency","precision"),  .funs = c(Mean="mean") )
#Accumulate

summarise_at( .vars = c("efficacy_Sd", "latency_Sd","precision_Sd"),
                                .funs = c(Mean="mean") )

bunchOfFitsMean<- function(df,numFitsForMean) {
  #Take a random subset of the fits
  whichFits<- sample(seq(1:reps), numFitsForMean, replace = TRUE)
  these<- filter(df, replicate == whichFits) #grab those particular fits
  #Calculate the mean fit
  meanThese<- these %>% group_by(subject,target,condition) %>%
          summarise_at( .vars = c("efficacy", "latency","precision"),  .funs = c(Mean="mean") )
  return(meanThese)
}

bootstrapFitSE<- function(dg,numFitsForMean, bootstrapSamples) {
  allTheFits<- data.frame()
  for (sampleI in 1:bootstrapSamples) {
    
    oneBunchOfFits<- bunchOfFitsMean(dg,numFitsForMean)
    allTheFits<- rbind(allTheFits,data.frame(oneBunchOfFits))
  }
  SEs<- allTheFits %>% summarise_at( .vars = c("efficacy_Mean", "latency_Mean","precision_Mean"),
                                .funs = c(SE="sd") )
  return(SEs)
}
#IS SE consistently going down but very little?

bootstrapSampleN<-20
numFitsForMean=2
x<-bootstrapFitSE(dfThis, numFitsForMean, bootstrapSampleN)

for (sampleSize in seq(1:reps/2)) {
  allTheFits<- data.frame()
  for (sampleI in 1:bootstrapSamples) {
    
    oneBunchOfFits<- bunchOfFitsMean(dfThis,2)
    allTheFits<- rbind(allTheFits,data.frame(oneBunchOfFits))
  }
  allTheFits %>% summarise_at( .vars = c("efficacy_Mean", "latency_Mean","precision_Mean"),
                                .funs = c(SE="sd") )


 #For each samplesize, bootstrap the SE 
  
#many<-dn %>% group_by(replicate,subject,target,condition) %>% 
#  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )
```

Check whether there is any condition with high variation - standard deviation greater than .05

```{r bad conditions, echo=FALSE, message=FALSE}
filter_at( w, vars(ends_with("Sd")), any_vars(. > .05) )

```
