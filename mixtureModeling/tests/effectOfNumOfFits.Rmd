---
title: "effect of number of fits"
author: "Alex Holcombe"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

I wrote this document to look at variation in parameter estimation across fit attempts, and along the way I found a weird multimodal-SPE subject that should be discarded probably. I also looked at how SE of parameter estimation decreased with number of fit attempts, but it may be not at all (unless something's wrong with the code), as seen in Part2. But of more interest is how mean neg log likelihood decreases with number of fits, which I should probably create a separate Rmd doc to study.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import some data as a test case - backwards paper 2, E1

```{r importData, echo=FALSE, message=FALSE}
#rm(list=ls())

#Compensate for path getting set to mixtureModeling/tests/
if (basename(getwd()) != "tests") {
  pathNeeded<- "mixtureModeling"
} else { 
  pathNeeded <- ".." 
}

data<- readRDS( file.path(pathNeeded,"tests", "alexImportBackwardsPaper2E1.Rdata") ) #.mat file been preprocessed into melted long dataframe
```


```{r fit, echo=FALSE, message=FALSE}

library(dplyr)
numItemsInStream<- length( data$letterSeq[1,] )  
df<-data
#to work with dplyr, can't have array field like letterSeq
df$letterSeq<- NULL
dg<- dplyr::filter(df,subject=="AA",condition==1,target==1)  #subject < "AC") #Otherwise will take long time to run the preliminary examples 

source(file.path(pathNeeded,"analyzeOneCondition.R"))
source(file.path(pathNeeded,"parameterBounds.R"))

numReplicates<-1
estimates<-dg %>% group_by(subject,target,condition) %>% 
  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )
```

Here are the estimates from one run for the one data sample:
```{r estimates, echo=FALSE, message=FALSE}
print(estimates)
```

Calculate the standard deviation of the fits for just one sample of data. 

```{r sd of fits, echo=FALSE, message=FALSE, cache=TRUE}

#For each condition, do the fit many times and calculate the standard deviation
#Maybe repeat the raw data dataframe as many times as want to fit. Then group by *replicate* and so get
#one row for each replicate fit.

reps<-20
#replicate raw data
dn<- df[rep(1:nrow(df),times = reps), ]
#add a rep field indicating replicate number
dn$replicate <- rep(1:reps, each=nrow(df))

numReplicates<-1 #Only fit the model once because we are manipulating number of fits with reps
estimates<-dn %>% group_by(replicate) %>% 
  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )

estimates<- estimates %>% rename(efficacy = p1, latency = p2, precision = p3)

#it automatically groups it using the tibble group specified variable, so get rid of that by changing to dataframe
estimates<-data.frame(estimates)
roundMean<- function(x) { round(mean(x),2) } #round the means so not so long to read
w<-estimates %>%  summarise_at( .vars = c("efficacy", "latency","precision"),
                                .funs = c(Mean="roundMean", Sd="sd") )
print(w)

```

The variation is remarkably low, except for precision?

Do it for each of many conditions to make sure it is true for each condition. 
For this part, doing 100 reps (fits) for each condition will take about an hour.

The variation for each condition is below. You'll note there's a lot of variance. That is, some subject*conditions have a high standard deviation.

```{r many conditions, echo=FALSE, message=FALSE, cache=TRUE}

#dg<-filter(df, subject<"AC")

reps<-100
#replicate raw data, creating reps copies of each condition*subject*
dWithManyDuplicatns<- df[rep(1:nrow(df),times = reps), ]
#add a rep field indicating replicate number
dWithManyDuplicatns$replicate <- rep(1:reps, each=nrow(df))
#save(dWithManyDuplicatns, file = "effectOfNumOfFits.RData")

numReplicates<-1 #Only fit the model once per function call because we are manipulating number of fits with reps, not numReplicates
###########
#Fit the data, which could take a very long time, because for each condition there are reps duplications
manyFits<-dWithManyDuplicatns %>% group_by(replicate,subject,target,condition) %>% 
  do( analyzeOneCondition(.,numItemsInStream,parameterBounds(),numReplicates) )

#do automatically groups it using the tibble group specified variable, so get rid of that by changing to dataframe
manyFits<-data.frame(manyFits)
manyFits<- manyFits %>% rename(efficacy = p1, latency = p2, precision = p3)
save(manyFits,file=file.path(pathNeeded,"tests","manyFits.Rdata")) #So can be used by Part2 to sample different subsets to get a sense of SD of mean for different subset sizes
  
#Calculate the mean and SD across all the replicates
manyMeanSD<-manyFits %>% group_by(subject,target,condition) %>% summarise_at( .vars = c("efficacy", "latency","precision"),
                                .funs = c(Mean="roundMean", Sd="sd") )
#it automatically groups it using the tibble group specified variable, so get rid of that by changing to dataframe
manyMeanSD<-data.frame(manyMeanSD)

dropMeans<- select(manyMeanSD, -ends_with("_mean"))  #Don't print the means as then can't see important stuff in single line
easyToReadSDs<- data.frame(lapply(dropMeans, function(y) if(is.numeric(y)) round(y, 2) else y)) #round numeric columns
print(easyToReadSDs) #SD for each condition
```

The mean sds across conditions are:
```{r mean sds, echo=FALSE, message=FALSE}

averageSD<- manyMeanSD %>% select(-ends_with("Mean")) %>% summarise_at( .vars = c("efficacy_Sd", "latency_Sd","precision_Sd"),
                                .funs = c(Mean="mean") )

print(averageSD)
```

To look into the discrepancy, check just the one subject-condition that had low variability.

```{r discrepancy, echo=FALSE, message=FALSE}

oneMeanSD<-manyFits %>% dplyr::filter(subject=="AA",condition==1,target==1) %>% summarise_at( .vars = c("efficacy", "latency","precision"),
                                .funs = c(Mean="roundMean", Sd="sd") )
print(oneMeanSD)
```

No discrepancy, so apparently it's just the one subject.

You'll notice the means across conditions are large for latency and precision. If results of fits were normally distributed, standard error when `r sampleSizes<-c(5,10,20,50)` 5, 10, 20, and 50 fits are done would be:

```{r assume normality, echo=FALSE, message=FALSE}
sampleSizes<-c(5,10,20,50)
SEnormalCalc<- averageSD[rep(1:nrow(averageSD),times = length(sampleSizes)), ]
SEnormalCalc$sampleSize <- sampleSizes
SEnormalCalc<-SEnormalCalc %>% mutate_each(funs(SE = ./sqrt(sampleSize)), -sampleSize)
SEnormalCalc[,4:7]
```

On this assumption of normal distribution of repeated fits, SE will go below .1 in all cases when do 20 replicates. But probably not normally distributed. Therefore, *in Part2.Rmd*,

determine the empirical SE by looking at the SD of the mean for different numbers of fits.

In other words for each `n` up to `reps`, take random samples of n from the replicates and
calculate the mean. We want to know the sd of the sampling distribution of that mean (the expected se). So we repeat the sampling many times and calculate the mean each time, and the sd of all those means.

Do it separately for each condition and then average the SEs of the conditions.

# Digression: weird Ss investigate

Subject AI, target=1,condition=2 has ridiculuous variance in latency (1.96) and precision (1.93) also for target=2, condition=1.  Plot below shows he has a second hill, which presumably is sometimes getting fit instead of the hill near SPE=0.

```{r weird Ss1, echo=FALSE, message=FALSE}

dAI<- dplyr::filter(df,subject=="AI",condition==2,target==1)  
minSPE<- -17; maxSPE<- 17
plotContinuousGaussian<-FALSE; annotateIt<-TRUE
source(file.path('..',"plotHistWithFit.R"))
plotHistWithFit(dAI,minSPE,maxSPE,dAI$targetSP,numItemsInStream,plotContinuousGaussian,annotateIt)
```
The other condition also looks multimodal, so very bad Gaussian fit.
```{r weird Ss2, echo=FALSE, message=FALSE}
dAI12<- dplyr::filter(df,subject=="AI",condition==1,target==2)  
plotHistWithFit(dAI12,minSPE,maxSPE,dAI$targetSP,numItemsInStream,plotContinuousGaussian,annotateIt)
```
